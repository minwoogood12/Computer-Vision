샘2 구조

에시로 100배치 (영상 100개)가 데이터로 사용 epoch는 총 100번 배치size = 1로 고정 영상프레임은 8로 고정
그럼 일단 총 100epoch반복, 100배치 반복해서
1개의 배치안에서 loss가 계산될거임
그럼 총 8개의 프레임으로 이루어진 즉 딕셔너리[logit[1, m, h, w] * 8] 이게 loss함수의 인풋으로 들어가고
GT도 [T, O, H, W]가 들어가서 이후 에측 m수에 맞게 복제되서 
[1, m, h, w]이거 하나당 로스값 하나씩 해서 총 8개의 로스를 더한걸 리턴
그 8개프레임의 총합 로스값을 1배치완료후 backward , update 
즉 100batch * 100epoch 총 10000번의 역전파실행 

loss_mask : 3개 마스크중 가장 높은거만 사용
loss_dice : 3개 마스크중 가장 높은거만 사용
loss_iou : 3개의 마스크의 평균 사용
-----------------------------------------

마스크프리vis구조

예시로 영상 16개가 데이터로 사용, 영상마다 16프레임일때 
입력은 image, GT가 ( image[3,h,w], GT [] ) * 16개의 리스트로 구성
모델에 입력되기 전에 16개의 영상 * 16프레임해서 256 * [3,h,w]로 flatten

이후 모델을 거쳐 나온 outputs은 
outputs = {
  "pred_logits": Tensor[B=16, Q=100, C+1=41],    # Classification logits
  "pred_masks":  Tensor[B=16, Q=100, H'=24, W'=32],  # Low-res mask predictions
  "aux_outputs": List[Dict] (길이 = decoder layers - 1)
}
즉 pred_masks 는 전체 영상 16개에 대한 쿼리수 100개에 해당하는 마스크를 가지고있음 = mask_logit
그리고 target은 딕셔너리 형태로 
targets = [
  {
    "labels": Tensor[N],                     # 클래스 인덱스 → ex: [0, 3, 4]
    "ids": Tensor[N, T],                     # 객체 id (프레임 간 identity 유지용)
    "masks": Tensor[N, T, H, W],             # 이진 GT 마스크 N :객체수 T : 프레임 수 
  },
  ...
]  # 총 16개 (비디오 개수만큼)
imagesS_lab_sim은 
images_lab_sim = 영상 개수 16 * 프레임수 16 만큼의 3차원 텐서 [h,w,커널크기-1]로 만들어진 리스트   
images_lab_sim_nei = 영상 개수 16 * 2차원 텐서[h*w,커널크기]로 만들어진 리스트 
images_lab_sim_nei1 = 
images_lab_sim_nei2 = 
images_lab_sim_nei3 = 
images_lab_sim_nei4 = 


batched_inputs: List[Dict], 길이 = 배치 크기 (예: 16개 비디오)

batched_inputs[i] = {
  "image": [Tensor[3, H, W] for _ in range(16)],     # 총 16 프레임, H=384, W=512
  "instances": [Instances for _ in range(16)],       # 각 프레임의 GT 마스크, 클래스 등
}




일단 그 샘2의 outputs, targets

outputs: Dict[str, Any] = {
    "multistep_pred_multimasks_high_res": List[Tensor of shape [B, M, H, W]], 프레임 개수만큼 리스트 만들어짐  b: 배치 개수 M은 마스크 예측 개수
    "multistep_pred_ious": List[Tensor of shape [B, M]],
    "multistep_object_score_logits": List[Tensor of shape [B, 1]],
    ...
}
targets: List[Tensor of shape [B, H, W]] 즉 배치수만큼 h,w존재 8개의 프레임만큼
이후 target_masks = targets.unsqueeze(1).float()  # => shape: [B, 1, H, W]로 확장됨

for src_masks, ious, object_score_logits in zip(
            src_masks_list, ious_list, object_score_logits_list
        ):
            self._update_losses(
                losses, src_masks, target_masks, ious, num_objects, object_score_logits
            )

여기에서 src_masks [B, M, H, W] 배치 수, 마스크 개수 이미지마다
target_masks [B, 1, H, W] -> 이후에 [B,M, H, W]같이 똑같이 expand로 복제되어서 비교 !

로스 리턴값 
{
    "loss_mask": tensor(3.42),
    "loss_dice": tensor(2.91),
    ...
}
이거는 _step으로 가서 core_loss만 추출해서 
self.scaler.scale(loss).backward()
loss_mts[loss_key].update(loss.item(), batch_size)
역전파, 업데이트 완료


maskfreevis

outputs = {
    "pred_logits": Tensor[B, num_queries, num_classes+1],  # classification
    "pred_masks":  Tensor[B, num_queries, H_mask, W_mask], # mask prediction
    "aux_outputs": [...],                                  # deep supervision (선택적)
}

targets = [
    {
        "labels": Tensor[N],         # instance class ID (GT 클래스)
        "ids": Tensor[N, T],         # instance tracking ID (프레임별 ID)
        "masks": Tensor[N, T, H, W]  # instance 마스크 (GT 마스크) n개의 객체 객체 수  T프레임에서
    },
    ...
]
로스 반환시 
losses = {
    "loss_ce": Tensor([1]),            # CrossEntropy loss
    "loss_mask": Tensor([1]),          # Mask BCE loss
    "loss_dice": Tensor([1]),          # Dice loss
    "loss_bound": Tensor([1]),         # 추가 손실들
    "loss_bound_neighbor": Tensor([1]),
    ...
}이런형태 이후 가중치를 적용해서
{
    "loss_ce": ...,
    "loss_mask": ...,
    "loss_dice": ...,
    ...
} 이런최종형태 이거는 dectectron내부로 
